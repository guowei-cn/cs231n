{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def affine_forward(x, w, b):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for an affine (fully-connected) layer.\n",
    "  The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "  examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "  reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "  then transform it to an output vector of dimension M.\n",
    "  Inputs:\n",
    "  - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "  - w: A numpy array of weights, of shape (D, M)\n",
    "  - b: A numpy array of biases, of shape (M,)\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - out: output, of shape (N, M)\n",
    "  - cache: (x, w, b)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "  # will need to reshape the input into rows.                                 #\n",
    "  #############################################################################\n",
    "  N = x.shape[0]\n",
    "  x_rsp = x.reshape(N, -1)\n",
    "  out = x_rsp.dot(w) + b\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, w, b)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for an affine layer.\n",
    "  Inputs:\n",
    "  - dout: Upstream derivative, of shape (N, M)\n",
    "  - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "  - dw: Gradient with respect to w, of shape (D, M)\n",
    "  - db: Gradient with respect to b, of shape (M,)\n",
    "  \"\"\"\n",
    "  x, w, b = cache\n",
    "  dx, dw, db = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the affine backward pass.                                 #\n",
    "  #############################################################################\n",
    "  N = x.shape[0]  \n",
    "  x_rsp = x.reshape(N , -1)  \n",
    "  dx = dout.dot(w.T)\n",
    "  dx = dx.reshape(*x.shape)\n",
    "  dw = x_rsp.T.dot(dout)\n",
    "  db = np.sum(dout, axis = 0)\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx, dw, db\n",
    "\n",
    "\n",
    "def relu_forward(x):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "  Input:\n",
    "  - x: Inputs, of any shape\n",
    "  Returns a tuple of:\n",
    "  - out: Output, of the same shape as x\n",
    "  - cache: x\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the ReLU forward pass.                                    #\n",
    "  #############################################################################\n",
    "  out = x * (x >= 0)\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = x\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "  Input:\n",
    "  - dout: Upstream derivatives, of any shape\n",
    "  - cache: Input x, of same shape as dout\n",
    "  Returns:\n",
    "  - dx: Gradient with respect to x\n",
    "  \"\"\"\n",
    "  dx, x = None, cache\n",
    "  #############################################################################\n",
    "  # TODO: Implement the ReLU backward pass.                                   #\n",
    "  #############################################################################\n",
    "  dx = (x >= 0) * dout\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx\n",
    "\n",
    "\n",
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "  \"\"\"\n",
    "  Forward pass for batch normalization.\n",
    "  \n",
    "  During training the sample mean and (uncorrected) sample variance are\n",
    "  computed from minibatch statistics and used to normalize the incoming data.\n",
    "  During training we also keep an exponentially decaying running mean of the mean\n",
    "  and variance of each feature, and these averages are used to normalize data\n",
    "  at test-time.\n",
    "  At each timestep we update the running averages for mean and variance using\n",
    "  an exponential decay based on the momentum parameter:\n",
    "  running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "  running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "  Note that the batch normalization paper suggests a different test-time\n",
    "  behavior: they compute sample mean and variance for each feature using a\n",
    "  large number of training images rather than using a running average. For\n",
    "  this implementation we have chosen to use running averages instead since\n",
    "  they do not require an additional estimation step; the torch7 implementation\n",
    "  of batch normalization also uses running averages.\n",
    "  Input:\n",
    "  - x: Data of shape (N, D)\n",
    "  - gamma: Scale parameter of shape (D,)\n",
    "  - beta: Shift paremeter of shape (D,)\n",
    "  - bn_param: Dictionary with the following keys:\n",
    "    - mode: 'train' or 'test'; required\n",
    "    - eps: Constant for numeric stability\n",
    "    - momentum: Constant for running mean / variance.\n",
    "    - running_mean: Array of shape (D,) giving running mean of features\n",
    "    - running_var Array of shape (D,) giving running variance of features\n",
    "  Returns a tuple of:\n",
    "  - out: of shape (N, D)\n",
    "  - cache: A tuple of values needed in the backward pass\n",
    "  \"\"\"\n",
    "  mode = bn_param['mode']\n",
    "  eps = bn_param.get('eps', 1e-5)\n",
    "  momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "  N, D = x.shape\n",
    "  running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "  running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "\n",
    "  out, cache = None, None\n",
    "  if mode == 'train':\n",
    "    #############################################################################\n",
    "    # TODO: Implement the training-time forward pass for batch normalization.   #\n",
    "    # Use minibatch statistics to compute the mean and variance, use these      #\n",
    "    # statistics to normalize the incoming data, and scale and shift the        #\n",
    "    # normalized data using gamma and beta.                                     #\n",
    "    #                                                                           #\n",
    "    # You should store the output in the variable out. Any intermediates that   #\n",
    "    # you need for the backward pass should be stored in the cache variable.    #\n",
    "    #                                                                           #\n",
    "    # You should also use your computed sample mean and variance together with  #\n",
    "    # the momentum variable to update the running mean and running variance,    #\n",
    "    # storing your result in the running_mean and running_var variables.        #\n",
    "    #############################################################################\n",
    "    sample_mean = np.mean(x, axis = 0)\n",
    "    sample_var = np.var(x , axis = 0)\n",
    "    x_hat = (x - sample_mean) / (np.sqrt(sample_var  + eps))\n",
    "    out = gamma * x_hat + beta\n",
    "    cache = (gamma, x, sample_mean, sample_var, eps, x_hat)\n",
    "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "    #pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "  elif mode == 'test':\n",
    "    #############################################################################\n",
    "    # TODO: Implement the test-time forward pass for batch normalization. Use   #\n",
    "    # the running mean and variance to normalize the incoming data, then scale  #\n",
    "    # and shift the normalized data using gamma and beta. Store the result in   #\n",
    "    # the out variable.                                                         #\n",
    "    #############################################################################\n",
    "    scale = gamma / (np.sqrt(running_var  + eps))\n",
    "    out = x * scale + (beta - running_mean * scale)\n",
    "    #pass\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "  else:\n",
    "    raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "\n",
    "  # Store the updated running means back into bn_param\n",
    "  bn_param['running_mean'] = running_mean\n",
    "  bn_param['running_var'] = running_var\n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Backward pass for batch normalization.\n",
    "  \n",
    "  For this implementation, you should write out a computation graph for\n",
    "  batch normalization on paper and propagate gradients backward through\n",
    "  intermediate nodes.\n",
    "  \n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of shape (N, D)\n",
    "  - cache: Variable of intermediates from batchnorm_forward.\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "  - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "  - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "  \"\"\"\n",
    "  dx, dgamma, dbeta = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the backward pass for batch normalization. Store the      #\n",
    "  # results in the dx, dgamma, and dbeta variables.                           #\n",
    "  #############################################################################\n",
    "  gamma, x, u_b, sigma_squared_b, eps, x_hat = cache\n",
    "  N = x.shape[0]\n",
    "\n",
    "  dx_1 = gamma * dout\n",
    "  dx_2_b = np.sum((x - u_b) * dx_1, axis=0)\n",
    "  dx_2_a = ((sigma_squared_b + eps) ** -0.5) * dx_1\n",
    "  dx_3_b = (-0.5) * ((sigma_squared_b + eps) ** -1.5) * dx_2_b\n",
    "  dx_4_b = dx_3_b * 1\n",
    "  dx_5_b = np.ones_like(x) / N * dx_4_b\n",
    "  dx_6_b = 2 * (x - u_b) * dx_5_b\n",
    "  dx_7_a = dx_6_b * 1 + dx_2_a * 1\n",
    "  dx_7_b = dx_6_b * 1 + dx_2_a * 1\n",
    "  dx_8_b = -1 * np.sum(dx_7_b, axis=0)\n",
    "  dx_9_b = np.ones_like(x) / N * dx_8_b\n",
    "  dx_10 = dx_9_b + dx_7_a\n",
    "\n",
    "  dgamma = np.sum(x_hat * dout, axis=0)\n",
    "  dbeta = np.sum(dout, axis=0)\n",
    "  dx = dx_10\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def batchnorm_backward_alt(dout, cache):\n",
    "  \"\"\"\n",
    "  Alternative backward pass for batch normalization.\n",
    "  \n",
    "  For this implementation you should work out the derivatives for the batch\n",
    "  normalizaton backward pass on paper and simplify as much as possible. You\n",
    "  should be able to derive a simple expression for the backward pass.\n",
    "  \n",
    "  Note: This implementation should expect to receive the same cache variable\n",
    "  as batchnorm_backward, but might not use all of the values in the cache.\n",
    "  \n",
    "  Inputs / outputs: Same as batchnorm_backward\n",
    "  \"\"\"\n",
    "  dx, dgamma, dbeta = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the backward pass for batch normalization. Store the      #\n",
    "  # results in the dx, dgamma, and dbeta variables.                           #\n",
    "  #                                                                           #\n",
    "  # After computing the gradient with respect to the centered inputs, you     #\n",
    "  # should be able to compute gradients with respect to the inputs in a       #\n",
    "  # single statement; our implementation fits on a single 80-character line.  #\n",
    "  #############################################################################\n",
    "  gamma, x, sample_mean, sample_var, eps, x_hat = cache\n",
    "  N = x.shape[0]\n",
    "  dx_hat = dout * gamma\n",
    "  dvar = np.sum(dx_hat* (x - sample_mean) * -0.5 * np.power(sample_var + eps, -1.5), axis = 0)\n",
    "  dmean = np.sum(dx_hat * -1 / np.sqrt(sample_var +eps), axis = 0) + dvar * np.mean(-2 * (x - sample_mean), axis =0)\n",
    "  dx = 1 / np.sqrt(sample_var + eps) * dx_hat + dvar * 2.0 / N * (x-sample_mean) + 1.0 / N * dmean\n",
    "  dgamma = np.sum(x_hat * dout, axis = 0)\n",
    "  dbeta = np.sum(dout , axis = 0)\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  \n",
    "  return dx, dgamma, dbeta\n",
    "\n",
    "\n",
    "def dropout_forward(x, dropout_param):\n",
    "  \"\"\"\n",
    "  Performs the forward pass for (inverted) dropout.\n",
    "  Inputs:\n",
    "  - x: Input data, of any shape\n",
    "  - dropout_param: A dictionary with the following keys:\n",
    "    - p: Dropout parameter. We drop each neuron output with probability p.\n",
    "    - mode: 'test' or 'train'. If the mode is train, then perform dropout;\n",
    "      if the mode is test, then just return the input.\n",
    "    - seed: Seed for the random number generator. Passing seed makes this\n",
    "      function deterministic, which is needed for gradient checking but not in\n",
    "      real networks.\n",
    "  Outputs:\n",
    "  - out: Array of the same shape as x.\n",
    "  - cache: A tuple (dropout_param, mask). In training mode, mask is the dropout\n",
    "    mask that was used to multiply the input; in test mode, mask is None.\n",
    "  \"\"\"\n",
    "  p, mode = dropout_param['p'], dropout_param['mode']\n",
    "  if 'seed' in dropout_param:\n",
    "    np.random.seed(dropout_param['seed'])\n",
    "\n",
    "  mask = None\n",
    "  out = None\n",
    "\n",
    "  if mode == 'train':\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the training phase forward pass for inverted dropout.   #\n",
    "    # Store the dropout mask in the mask variable.                            #\n",
    "    ###########################################################################\n",
    "    mask = (np.random.rand(*x.shape) >= p) / (1 - p)\n",
    "    #mask = (np.random.rand(x.shape[1]) >= p) / (1 - p)\n",
    "    out = x * mask\n",
    "    #pass\n",
    "    ###########################################################################\n",
    "    #                            END OF YOUR CODE                             #\n",
    "    ###########################################################################\n",
    "  elif mode == 'test':\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the test phase forward pass for inverted dropout.       #\n",
    "    ###########################################################################\n",
    "    out = x\n",
    "    #pass\n",
    "    ###########################################################################\n",
    "    #                            END OF YOUR CODE                             #\n",
    "    ###########################################################################\n",
    "\n",
    "  cache = (dropout_param, mask)\n",
    "  out = out.astype(x.dtype, copy=False)\n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Perform the backward pass for (inverted) dropout.\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of any shape\n",
    "  - cache: (dropout_param, mask) from dropout_forward.\n",
    "  \"\"\"\n",
    "  dropout_param, mask = cache\n",
    "  mode = dropout_param['mode']\n",
    "  \n",
    "  dx = None\n",
    "  if mode == 'train':\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the training phase backward pass for inverted dropout.  #\n",
    "    ###########################################################################\n",
    "    dx = dout * mask\n",
    "    #pass\n",
    "    ###########################################################################\n",
    "    #                            END OF YOUR CODE                             #\n",
    "    ###########################################################################\n",
    "  elif mode == 'test':\n",
    "    dx = dout\n",
    "  return dx\n",
    "\n",
    "\n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "  \"\"\"\n",
    "  A naive implementation of the forward pass for a convolutional layer.\n",
    "  The input consists of N data points, each with C channels, height H and width\n",
    "  W. We convolve each input with F different filters, where each filter spans\n",
    "  all C channels and has height HH and width HH.\n",
    "  Input:\n",
    "  - x: Input data of shape (N, C, H, W)\n",
    "  - w: Filter weights of shape (F, C, HH, WW)\n",
    "  - b: Biases, of shape (F,)\n",
    "  - conv_param: A dictionary with the following keys:\n",
    "    - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "      horizontal and vertical directions.\n",
    "    - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "  Returns a tuple of:\n",
    "  - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "    H' = 1 + (H + 2 * pad - HH) / stride\n",
    "    W' = 1 + (W + 2 * pad - WW) / stride\n",
    "  - cache: (x, w, b, conv_param)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the convolutional forward pass.                           #\n",
    "  # Hint: you can use the function np.pad for padding.                        #\n",
    "  #############################################################################\n",
    "  N, C, H, W = x.shape\n",
    "  F, _, HH, WW = w.shape\n",
    "  stride, pad = conv_param['stride'], conv_param['pad']\n",
    "  H_out = 1 + (H + 2 * pad - HH) / stride\n",
    "  W_out = 1 + (W + 2 * pad - WW) / stride\n",
    "  out = np.zeros((N , F , H_out, W_out))\n",
    "\n",
    "  x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n",
    "  for i in range(H_out):\n",
    "      for j in range(W_out):\n",
    "          x_pad_masked = x_pad[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW]\n",
    "          for k in range(F):\n",
    "              out[:, k , i, j] = np.sum(x_pad_masked * w[k, :, :, :], axis=(1,2,3))\n",
    "          #out[:, : , i, j] = np.sum(x_pad_masked * w[:, :, :, :], axis=(1,2,3))\n",
    "          \n",
    "  #for k in range(F):\n",
    "      #out[:, k, :, :] = out[:, k, :, :] + b[k]\n",
    "  out = out + (b)[None, :, None, None]\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, w, b, conv_param)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def conv_backward_naive(dout, cache):\n",
    "  \"\"\"\n",
    "  A naive implementation of the backward pass for a convolutional layer.\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives.\n",
    "  - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to x\n",
    "  - dw: Gradient with respect to w\n",
    "  - db: Gradient with respect to b\n",
    "  \"\"\"\n",
    "  dx, dw, db = None, None, None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the convolutional backward pass.                          #\n",
    "  #############################################################################\n",
    "  x, w, b, conv_param = cache\n",
    "  \n",
    "  N, C, H, W = x.shape\n",
    "  F, _, HH, WW = w.shape\n",
    "  stride, pad = conv_param['stride'], conv_param['pad']\n",
    "  H_out = 1 + (H + 2 * pad - HH) / stride\n",
    "  W_out = 1 + (W + 2 * pad - WW) / stride\n",
    "  \n",
    "  x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n",
    "  dx = np.zeros_like(x)\n",
    "  dx_pad = np.zeros_like(x_pad)\n",
    "  dw = np.zeros_like(w)\n",
    "  db = np.zeros_like(b)\n",
    "  \n",
    "  db = np.sum(dout, axis = (0,2,3))\n",
    "  \n",
    "  x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n",
    "  for i in range(H_out):\n",
    "      for j in range(W_out):\n",
    "          x_pad_masked = x_pad[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW]\n",
    "          for k in range(F): #compute dw\n",
    "              dw[k ,: ,: ,:] += np.sum(x_pad_masked * (dout[:, k, i, j])[:, None, None, None], axis=0)\n",
    "          for n in range(N): #compute dx_pad\n",
    "              dx_pad[n, :, i*stride:i*stride+HH, j*stride:j*stride+WW] += np.sum((w[:, :, :, :] * \n",
    "                                                 (dout[n, :, i, j])[:,None ,None, None]), axis=0)\n",
    "  dx = dx_pad[:,:,pad:-pad,pad:-pad]\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx, dw, db\n",
    "\n",
    "\n",
    "def max_pool_forward_naive(x, pool_param):\n",
    "  \"\"\"\n",
    "  A naive implementation of the forward pass for a max pooling layer.\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C, H, W)\n",
    "  - pool_param: dictionary with the following keys:\n",
    "    - 'pool_height': The height of each pooling region\n",
    "    - 'pool_width': The width of each pooling region\n",
    "    - 'stride': The distance between adjacent pooling regions\n",
    "  Returns a tuple of:\n",
    "  - out: Output data\n",
    "  - cache: (x, pool_param)\n",
    "  \"\"\"\n",
    "  out = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the max pooling forward pass                              #\n",
    "  #############################################################################\n",
    "  N, C, H, W = x.shape\n",
    "  HH, WW, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride']\n",
    "  H_out = (H-HH)/stride+1\n",
    "  W_out = (W-WW)/stride+1\n",
    "  out = np.zeros((N,C,H_out,W_out))\n",
    "  for i in xrange(H_out):\n",
    "        for j in xrange(W_out):\n",
    "            x_masked = x[:,:,i*stride : i*stride+HH, j*stride : j*stride+WW]\n",
    "            out[:,:,i,j] = np.max(x_masked, axis=(2,3)) \n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  cache = (x, pool_param)\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def max_pool_backward_naive(dout, cache):\n",
    "  \"\"\"\n",
    "  A naive implementation of the backward pass for a max pooling layer.\n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives\n",
    "  - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "  Returns:\n",
    "  - dx: Gradient with respect to x\n",
    "  \"\"\"\n",
    "  dx = None\n",
    "  #############################################################################\n",
    "  # TODO: Implement the max pooling backward pass                             #\n",
    "  #############################################################################\n",
    "  x, pool_param = cache\n",
    "  N, C, H, W = x.shape\n",
    "  HH, WW, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride']\n",
    "  H_out = (H-HH)/stride+1\n",
    "  W_out = (W-WW)/stride+1\n",
    "  dx = np.zeros_like(x)\n",
    "  \n",
    "  for i in xrange(H_out):\n",
    "     for j in xrange(W_out):\n",
    "        x_masked = x[:,:,i*stride : i*stride+HH, j*stride : j*stride+WW]\n",
    "        max_x_masked = np.max(x_masked,axis=(2,3))\n",
    "        temp_binary_mask = (x_masked == (max_x_masked)[:,:,None,None])\n",
    "        dx[:,:,i*stride : i*stride+HH, j*stride : j*stride+WW] += temp_binary_mask * (dout[:,:,i,j])[:,:,None,None]\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "  return dx\n",
    "\n",
    "\n",
    "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
    "  \"\"\"\n",
    "  Computes the forward pass for spatial batch normalization.\n",
    "  \n",
    "  Inputs:\n",
    "  - x: Input data of shape (N, C, H, W)\n",
    "  - gamma: Scale parameter, of shape (C,)\n",
    "  - beta: Shift parameter, of shape (C,)\n",
    "  - bn_param: Dictionary with the following keys:\n",
    "    - mode: 'train' or 'test'; required\n",
    "    - eps: Constant for numeric stability\n",
    "    - momentum: Constant for running mean / variance. momentum=0 means that\n",
    "      old information is discarded completely at every time step, while\n",
    "      momentum=1 means that new information is never incorporated. The\n",
    "      default of momentum=0.9 should work well in most situations.\n",
    "    - running_mean: Array of shape (D,) giving running mean of features\n",
    "    - running_var Array of shape (D,) giving running variance of features\n",
    "    \n",
    "  Returns a tuple of:\n",
    "  - out: Output data, of shape (N, C, H, W)\n",
    "  - cache: Values needed for the backward pass\n",
    "  \"\"\"\n",
    "  out, cache = None, None\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Implement the forward pass for spatial batch normalization.         #\n",
    "  #                                                                           #\n",
    "  # HINT: You can implement spatial batch normalization using the vanilla     #\n",
    "  # version of batch normalization defined above. Your implementation should  #\n",
    "  # be very short; ours is less than five lines.                              #\n",
    "  #############################################################################\n",
    "  N, C, H, W = x.shape\n",
    "  temp_output, cache = batchnorm_forward(x.transpose(0,3,2,1).reshape((N*H*W,C)), gamma, beta, bn_param)\n",
    "  out = temp_output.reshape(N,W,H,C).transpose(0,3,2,1)\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  return out, cache\n",
    "\n",
    "\n",
    "def spatial_batchnorm_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Computes the backward pass for spatial batch normalization.\n",
    "  \n",
    "  Inputs:\n",
    "  - dout: Upstream derivatives, of shape (N, C, H, W)\n",
    "  - cache: Values from the forward pass\n",
    "  \n",
    "  Returns a tuple of:\n",
    "  - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
    "  - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
    "  - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
    "  \"\"\"\n",
    "  dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "  #############################################################################\n",
    "  # TODO: Implement the backward pass for spatial batch normalization.        #\n",
    "  #                                                                           #\n",
    "  # HINT: You can implement spatial batch normalization using the vanilla     #\n",
    "  # version of batch normalization defined above. Your implementation should  #\n",
    "  # be very short; ours is less than five lines.                              #\n",
    "  #############################################################################\n",
    "  N,C,H,W = dout.shape\n",
    "  dx_temp, dgamma, dbeta = batchnorm_backward_alt(dout.transpose(0,3,2,1).reshape((N*H*W,C)),cache)\n",
    "  dx = dx_temp.reshape(N,W,H,C).transpose(0,3,2,1)\n",
    "  #pass\n",
    "  #############################################################################\n",
    "  #                             END OF YOUR CODE                              #\n",
    "  #############################################################################\n",
    "\n",
    "  return dx, dgamma, dbeta\n",
    "  \n",
    "\n",
    "def svm_loss(x, y):\n",
    "  \"\"\"\n",
    "  Computes the loss and gradient using for multiclass SVM classification.\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "  - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar giving the loss\n",
    "  - dx: Gradient of the loss with respect to x\n",
    "  \"\"\"\n",
    "  N = x.shape[0]\n",
    "  correct_class_scores = x[np.arange(N), y]\n",
    "  margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)\n",
    "  margins[np.arange(N), y] = 0\n",
    "  loss = np.sum(margins) / N\n",
    "  num_pos = np.sum(margins > 0, axis=1)\n",
    "  dx = np.zeros_like(x)\n",
    "  dx[margins > 0] = 1\n",
    "  dx[np.arange(N), y] -= num_pos\n",
    "  dx /= N\n",
    "  return loss, dx\n",
    "\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "  \"\"\"\n",
    "  Computes the loss and gradient for softmax classification.\n",
    "  Inputs:\n",
    "  - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "  - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar giving the loss\n",
    "  - dx: Gradient of the loss with respect to x\n",
    "  \"\"\"\n",
    "  probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "  probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "  N = x.shape[0]\n",
    "  loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
    "  dx = probs.copy()\n",
    "  dx[np.arange(N), y] -= 1\n",
    "  dx /= N\n",
    "  return loss, dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
